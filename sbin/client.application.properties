# This is a sample client properties file, with some default values. Please update this with your own appropriate settings.

conf.hadoop.home=
conf.hadoop.conf-dir=
conf.hadoop.core-site=
conf.hadoop.hdfs-site=
conf.hadoop.mapred-site=
conf.hadoop.yarn-site=


client.app.jar=/<PATH_TO>/datory-pp-core-1.01-jar-with-dependencies.jar
client.app.main.class=Application
client.app.args=

client.spark.app.name=Datory ETL Preprocessing Application (AMH)
client.spark.master=yarn
client.spark.deployment.mode=client
client.spark.home=/<PATH_TO>/cloudera/parcels/SPARK2/lib/spark2
client.spark.driver.memory=8g
client.spark.executor.memory=16g
client.spark.executor.cores=8
client.spark.executor.num=4
client.spark.verbose=true
client.spark.yarn.keytab=<PATH_TO_KEYTAB>
client.spark.yarn.principal=<YOUR_KEYTAB_PRINCIPAL>
client.spark.extra.jars=
client.spark.job.listener.timeout.secs=900
client.spark.driver.java.options=-Djava.security.auth.login.config=/var/opt/<PATH_TO_JAAS_CONF>/jaas.conf

# Properties files that will be sent as argument to cluster Spark App
client.cluster.application.properties=./cluster.application.properties




# Configure this based on the size of your input file, etc.
client.spark.executor.extra.java.options=-XX:+UseG1GC -XX:InitiatingHeapOccupancyPercent=35 -XX:ConcGCThreads=12 
client.spark.executor.extra.classpath=
client.spark.executor.extra.library.path=
client.spark.driver.extra.classpath=
client.spark.driver.extra.library.path=

client.spark.serializer=org.apache.serializer.KryoSerializer

# for Java Serializer
client.spark.serializer.objectStreamReset=100

# for Kryo Serializer
client.spark.kryoserializer.buffer.max=1900m